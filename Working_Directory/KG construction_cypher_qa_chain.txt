https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/

Packages: Neo4jGraph, GraphCypherQAChain

1. Importing Required Libraries
from langchain_neo4j import Neo4jGraph, GraphCypherQAChain
from langchain_core.prompts.prompt import PromptTemplate

2. Connect to Neo4j
graph = Neo4jGraph(url="bolt://localhost:7687", username="neo4j", password="password")

3. Seed the Database
graph.query("""
MERGE (m:Movie {name:"Top Gun", runtime: 120})
WITH m UNWIND ["Tom Cruise", "Val Kilmer"] AS actor
MERGE (a:Actor {name:actor})
MERGE (a)-[:ACTED_IN]->(m)
""")

4. Refresh Schema (If the schema of the database changes)
graph.refresh_schema()
print(graph.schema)

5. Enhanced Schema Information (With enhanced schema options); validate cypher, Limit Results, Returning Intermediate Steps
enhanced_graph = Neo4jGraph(url="bolt://localhost:7687", username="neo4j", password="password", enhanced_schema=True)
print(enhanced_graph.schema)

6. Query the Graph with LLMs (Separate LLMs for Cypher & Answer Generation)
chain = GraphCypherQAChain.from_llm(
    cypher_llm=llm1, qa_llm="llm2", graph=graph, verbose=True,
    validate_cypher=True,
    allow_dangerous_requests=True,
    top_k=2,
    return_intermediate_steps=True
)

result = chain.invoke({"query": "Who played in Top Gun?"})
print(result["intermediate_steps"])
print(result)

7. Returning Direct Results
chain = GraphCypherQAChain.from_llm(
    cypher_llm=llm1, qa_llm="llm2", graph=graph, verbose=True,
    validate_cypher=True,
    allow_dangerous_requests=True,
    top_k=2,
    return_direct=True
)

8. Custom Cypher Templates with Few-shot Prompting

TEMPLATE = """Schema: {schema}
# Example: How many people acted in Top Gun?
MATCH (m:Movie {{name:"Top Gun"}})<-[:ACTED_IN]-() RETURN count(*) AS numberOfActors
The question: {question}
"""

# Create the prompt template
PROMPT = PromptTemplate(input_variables=["schema", "question"], template=TEMPLATE)

# Initialize the chain with custom Cypher prompt
chain = GraphCypherQAChain.from_llm(
    llm=llm, 
    graph=graph, 
    cypher_prompt=PROMPT
)




Construction of knowledge graph and storing in graph database: 
https://python.langchain.com/v0.1/docs/use_cases/graph/constructing/#storing-to-graph-database
<Packages:LLMGraphTransformer,Neo4jGraph>

# Importing Required Libraries
import pdfplumber
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_neo4j import Neo4jGraph

# Extract Text from the PDF
with pdfplumber.open("work.pdf") as pdf:
    text = ""
    for page in pdf.pages:
        text += page.extract_text()

# Split Text into Smaller Chunks
recursive_text_splitter = RecursiveCharacterTextSplitter()
chunks = recursive_text_splitter.split_text(text)

# 1. Initialize LLM and Graph Transformer
llm = ChatOpenAI(temperature=0, model_name="gpt-4-turbo")
llm_transformer = LLMGraphTransformer(llm=llm)

# 2. Process Each Chunk and Generate Graph Data
graph_documents = []
for chunk in chunks:
    documents = [Document(page_content=chunk)]
    graph_documents_chunk = llm_transformer.convert_to_graph_documents(documents)
    
    # Print the generated nodes and relationships (for debugging/verification)
    print(f"Nodes: {graph_documents_chunk[0].nodes}")
    print(f"Relationships: {graph_documents_chunk[0].relationships}")
    
    # Collect all graph documents
    graph_documents.extend(graph_documents_chunk)

# 6. Connect to Neo4j Database and Store the Graph Data
graph = Neo4jGraph(url="bolt://localhost:7687", username="neo4j", password="password")
graph.add_graph_documents(graph_documents)
